<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE concept PUBLIC "-//OASIS//DTD DITA Concept//EN" "http://docs.oasis-open.org/dita/v1.1/OS/dtd/concept.dtd">
<concept id="transactions_by_region_type"> 
  <title>Transactions by Region Type</title> 
  <prolog><metadata><keywords><keyword>gemfire transactions partitioned
			 region</keyword><keyword>gemfire transactions partition
			 region</keyword><keyword>gemfire transaction
			 initiator</keyword><keyword>gemfire transactional data
			 host</keyword><keyword>gemfire transactions replicated
			 region</keyword><keyword>gemfire transactions distributed-ack
			 regions</keyword><keyword>gemfire transactions distributed-no-ack
			 region</keyword><keyword>gemfire conflicting
			 transactions</keyword></keywords></metadata>
  </prolog> 
  <conbody> 
	 <section id="section_5942356A8AF2405BB206AC0530BE6279"> 
		<p>A transaction is managed on a per-cache basis, so multiple regions in
		  the cache can participate in a single transaction. The data scope of a vFabric GemFire
		  cache transaction is the cache that hosts the transactional data. For
		  partitioned regions, this may be a remote host to the host running the
		  transaction application. Any transaction that includes one or more partitioned
		  regions is run on the member storing the primary copy of the partitioned region
		  data. Otherwise, the transaction host is the same one running the application. 
		<ul id="ul_9492E4440F444D688101207C65E442D7"> 
		  <li id="li_CC220513D4DA475F8C97A23BDA12DE54">The member running the
			 transaction code is called the transaction initiator. 
		  </li> 
		  <li id="li_20F393091CE74C19B083FC98694941CC">The member that hosts the
			 data—and the transaction—is called the transactional data host. 
		  </li> 
		</ul> 
		</p> 
		<p>So the transactional data host may be local or remote to the
		  transaction initiator. In either case, when the transaction commits, data
		  distribution is done from the transactional data host in the same way. 
		</p> 
		<p> 
		  <draft-comment>This org needs some attention - the "initiator/host"
			 discussion should be moved up. It pertains to client transactions as well, as
			 of 6.6. This should be in "How Txs work" directly. I think. JP 
		  </draft-comment> 
		</p> 
	 </section> 
	 <section id="section_E64618FB6D234276BC443D19F9B76AA8"> 
		<title>Transactions and Partitioned Regions</title> 
		<p>In partitioned regions, transaction operations are done first on the
		  primary data store then distributed to other members from there, regardless of
		  which member initializes the cache operation. This is the same as is done for
		  normal cache operations on partitioned regions. 
		</p> 
		<p>In this figure, M1 runs two transactions. 
		<ul id="ul_889FCAFEE4F9470EAAB472FF24A4FA52"> 
		  <li id="li_DF9EBE26A1284703B61804A54D4200A7">The first, T1, works on
			 data whose primary buckets are stored in M1, so M1 is both initiator and data
			 host for the transaction. 
		  </li> 
		  <li id="li_AC8D7C1254BE469692700B22C40EF376">The second transaction,
			 T2, works on data whose primary buckets are stored in M2, so M1 is the
			 transaction initiator and M2 is the transactional data host. 
		  </li> 
		</ul><i>Transaction on a Partitioned Region:</i> 
		</p> 
		<p> 
		  <image placement="break" id="image_9BF680072A674BCF9F01958753F02952"
			href="../../common/images/transactions-partitioned-1.png" align="left"/> 
		</p> 
		<p>The transaction is managed on the data host. This includes the
		  transactional view, all operations, and all local cache event handling. In the
		  figure, when T2 is committed, the cache on M2 is updated and the transaction
		  events distributed throughout the system, exactly as if the transaction had
		  originated on M2. 
		</p> 
		<p>The first region operation in the transaction determines the
		  transactional data host. All other operations must also work with that as their
		  transactional data host: 
		<ul id="ul_BE4E509EF82A44889E7DDDA6124DFF2E"> 
		  <li id="li_04A4349715BB4486A252AF8D162BFBF7">All partitioned region
			 data managed inside the transaction must use the transactional data host as
			 their primary data store. In the figure above, if transaction T2 tried to put
			 entry W or transaction T1 tried to put entry Z, they would get a 
			 <codeph>TransactionDataNotColocatedException</codeph>. For
			 information on partitioning your data so it is grouped properly for your
			 transactions, see 
			 <xref
			  href="../partitioned_regions/custom_partitioning_and_data_colocation.xml#custom_partitioning_and_data_colocation"
			  type="concept" format="dita" scope="local"><?xm-replace_text Understanding Custom Partitioning and Data Colocation?></xref>.
			 In addition, the data must not be moved during the transaction. Plan any
			 partitioned region rebalancing to avoid rebalancing while transactions are
			 running. See 
			 <xref
			  href="../partitioned_regions/rebalancing_pr_data.xml#rebalancing_pr_data"
			  type="concept" format="dita" scope="local"><?xm-replace_text Rebalancing Partitioned Region Data?></xref>.
			 
		  </li> 
		  <li id="li_830704A3C60C4AB490A43348B8050AA0">All non-partitioned region
			 data managed inside the transaction must be available on the transactional data
			 host and must be distributed. Operations on regions with local scope are not
			 allowed in transactions with partitioned regions. 
		  </li> 
		</ul> 
		</p> 
		<p>The next figure shows a transaction that uses two partitioned regions
		  and one replicated region. As with the single region example, all local event
		  handling is done on the transactional data host. 
		</p> 
		<p>For a transaction in these data keys to work, the first operation must
		  be on one of the partitioned regions, to establish M2 as the transactional data
		  host. Running the first operation on a key in the replicated region would
		  establish M1 as the transactional data host, and subsequent operations on the
		  partitioned region data would fail with a 
		  <codeph>TransactionDataNotColocated</codeph> exception. 
		</p> 
		<p><i>Transaction on a Partitioned Region with Other Regions:</i> 
		</p> 
		<p> 
		  <image placement="break"
			href="../../common/images/transactions-partitioned-2.png"
			id="image_34496249618F46F8B8F7E2D4F342E1E6"/> 
		</p> 
	 </section> 
	 <section id="section_C55E80C7136D4A9A8327563E4B89356D"> 
		<title>Transactions and Replicated Regions</title> 
		<p>For replicated regions, the transaction and its operations are applied
		  to the local member and the resulting transaction state is distributed to other
		  members according to the attributes of each region. 
		</p> 
		<note>If possible, use 
		  <codeph>distributed-ack</codeph> scope for your regions where you will
		  run transactions. The 
		  <codeph>REPLICATE</codeph> region shortcuts use 
		  <codeph>distributed-ack</codeph> scope. 
		</note> 
		<p>The region’s scope affects how data is distributed during the commit
		  phase. Transactions are supported for these region scopes: 
		<ul id="ul_3FEEBD3A9B1A4033A3D1B943250E904E"> 
		  <li
			id="li_4CBDE8D31C7D4D56866841B993C15BD9"><codeph><b>distributed-ack</b></codeph>.
			 Handles transactional conflicts both locally and between members. The 
			 <codeph>distributed-ack</codeph> scope is designed to protect data
			 consistency. This scope provides the highest level of coordination among
			 transactions in different members. When the commit call returns for a
			 transaction run on all distributed-ack regions, you can be sure that the
			 transaction’s changes have already been sent and processed. In addition, any
			 callbacks in the remote member have been invoked. 
		  </li> 
		  <li
			id="li_68A95B28168D4DF2A353F7B88953EB48"><codeph><b>distributed-no-ack</b></codeph>.
			 Handles transactional conflicts locally, less coordination between members.
			 This provides the fastest transactions with distributed regions, but doesn't
			 work for all situations. This scope is appropriate for: 
			 <ul id="ul_796D59016AF64CCC81B8760B80DAF407"> 
				<li id="li_C8E4299096654708BC63864F809EEF35">Applications with only
				  one writer 
				</li> 
				<li id="li_BA9199984E7340578D9997DC1A8ACC90">Applications with
				  multiple writers that write to different data sets 
				</li> 
			 </ul> 
		  </li> 
		  <li
		  id="li_FFC24384D3F043719EF376DA0F5E1922"><codeph><b>local</b></codeph>. No
			 distribution, handles transactional conflicts locally. Transactions on regions
			 with local scope have no distribution, but they perform conflict checks in the
			 local member. You can have conflict between two threads when their transactions
			 change the same entry, like object Y in this figure. 
		  </li> 
		</ul> 
		</p>
		<p>Transactions on non-replicated regions (regions that use the old API
		  with DataPolicy EMPTY, NORMAL and PRELOADED) are always transaction initiators,
		  and the transaction data host is always a member with a replicated region. This
		  is similar to the way transactions using the PARTITION_PROXY shortcut are
		  forwarded to members with primary bucket. 
		</p> 
		<p> 
		  <note>When you have transactions operating on EMPTY, NORMAL or
			 PARTITION regions, make sure that the GemFire property 
			 <codeph>conserve-sockets</codeph> is set to false to avoid
			 distributed deadlocks. An empty region is a region created with the API 
			 <codeph>RegionShortcut.REPLICATE_PROXY</codeph> or a region with that
			 uses the old API of 
			 <codeph>DataPolicy</codeph> set to 
			 <codeph>EMPTY</codeph>. 
		  </note> 
		</p> 
	 </section> 
	 <section id="section_A21530FFD1BD44F7BA78E98790F6EB12"> 
		<title>Conflicting Transactions in Distributed-Ack Regions</title> 
		<p>In this series of figures, even after the commit operation is
		  launched, the transaction continues to exist during the data distribution (step
		  3). The commit does not complete until the changes are made in the remote
		  caches and the application in M1 receives the callbacks to verify that the
		  tasks are complete. 
		<ul id="ul_4A1671AA6F904C78A18C55013013ED14"> 
		  <li id="li_6DE2DD83F533485A93C18ECB59728C44"><b>Step 1:</b> Before
			 commit, Transactions T1 and T2 each change the same entry in Region B within
			 their local cache. T1 also makes a change to Region A.<image placement="break"
			 id="image_B4F44906F6364948B0B34C8B8667A95A"
			 href="../../common/images/transactions-replicate-1.png"></image> 
		  </li> 
		  <li id="li_4B1E299BABA2401681CCA984B5BAFD41"><b>Step 2:</b> Conflict
			 detected and eliminated. The distributed system recognizes the potential
			 conflict from Transactions T1 and T2 using the same entry. T1 started to commit
			 first, so it is allowed to continue. T2's commit fails with a conflict.<image
			 placement="break" id="image_94C7994E1E924F0580562FA3C94718A9"
			 href="../../common/images/transactions-replicate-2.png"></image> 
		  </li> 
		  <li id="li_48D054B728524CDD8E285075CB18F080"><b>Step 3:</b> Changes are
			 in transit. T1 commits and its changes are merged into the local cache. The
			 commit does not complete until GemFire
			 distributes the changes to the remote regions and acknowledgment is
			 received.<image placement="break" id="image_139D8BAC38FA4888A7BCEC9EE9B9478C"
			 href="../../common/images/transactions-replicate-3.png"></image> 
		  </li> 
		  <li id="li_00A2D612F5D7485889986EAB7D122BB1"><b>Step 4:</b> After
			 commit. Region A in M2 and Region B in M3 reflect the changes from transaction
			 T1 and M1 has received acknowledgment. Results may not be identical in
			 different members if their region attributes (such as expiration) are
			 different.<image placement="break" id="image_9D41EDB2C7404395BEADEBD8B1715C63"
			 href="../../common/images/transactions-replicate-4.png"></image> 
		  </li> 
		</ul> 
		</p> 
	 </section> 
	 <section id="section_F85E95BE8D394FBE822170A119A9730E"> 
		<title>Conflicting Transactions in Distributed-No-Ack Regions</title> 
		<p>These figures show how using the no-ack scope can produce unexpected
		  results. These two transactions are operating on the same data point, in region
		  B. Since they use no-ack scope, the conflicting changes cross paths and leave
		  the data in an inconsistent state. 
		<ul id="ul_4588447AD2CC4737B2FA3D3FE954ACCC"> 
		  <li id="li_1BB80B3B5D6B49D486F532036B1F383B"><b>Step 1:</b> As in the
			 previous example, Transactions T1 and T2 each change the same entry in Region B
			 within their local cache. T1 also makes a change to Region A. However, neither
			 commit fails.<image placement="break"
			 id="image_EEB0F293BAD24842967D5475FEBA936F"
			 href="../../common/images/transactions-replicate-1.png"></image> 
		  </li> 
		  <li id="li_56F739393CA54D56AF8518966A11E73E"><b>Step 2:</b> Changes are
			 in transit. Transactions T1 and T2 commit and merge their changes into the
			 local cache. GemFire
			 then distributes changes to the remote regions.<image placement="break"
			 id="image_5923C5B75D8145799BE918765DE7FAE9"
			 href="../../common/images/transactions-replicate-no-ack-1.png"></image> 
		  </li> 
		  <li id="li_28DF9C0ECFF04D0C8DB99C03B6BE3377"><b>Step 3:</b>
			 Distribution is complete. The non-conflicting changes in Region A have been
			 distributed to M2 as expected. For Region B however, T1 and T2 have traded
			 changes, which is probably not the intended result.<image placement="break"
			 id="image_FA7C3BE7F51A4BF4B68253E98164DB28"
			 href="../../common/images/transactions-replicate-no-ack-2.png"></image> 
		  </li> 
		</ul> 
		</p> 
	 </section> 
	 <section id="section_760DE9F2226B46AD8A025F562CEA4D40"> 
		<title>Conflicting Transactions with Local Scope</title> 
		<p> When encountering conflicts with local scope, the first transaction
		  to start the commit process "wins." The other transaction’s commit fails with a
		  conflict, and its changes are dropped. In the diagram below, the resulting
		  value for entry "Y"
		  depends on which transaction commits first.<image placement="break"
		  id="image_A37172C328404796AE1F318068C18F43"
		  href="../../common/images/transactions-replicate-local-1.png"></image> 
		</p> 
	 </section> 
	 <section id="section_1F8F0BC54BC547099F7F1F5C5002D4CA"> 
		<title>Transactions and Persistent Regions</title> 
		<p>By default, GemFire does not allow transactions on persistent regions.
		  You can enable the use of transactions on persistent regions by setting the
		  gemfire property 
		  <codeph>gemfire.ALLOW_PERSISTENT_TRANSACTIONS</codeph> to 
		  <codeph>true</codeph>. 
		</p> 
		<p>When executing transactions on persistent regions, we recommend using
		  the 
		  <codeph>TransactionWriter</codeph> to log all transactions along with a
		  time stamp. This will allow you to recover in the event that all nodes fail
		  simultaneously while a transaction is being committed. You can use the log to
		  recover the data manually. 
		</p> 
		<p> 
		</p> 
	 </section> 
  </conbody> 
</concept> 
