<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE concept PUBLIC "-//OASIS//DTD DITA Concept//EN" "http://docs.oasis-open.org/dita/v1.1/OS/dtd/concept.dtd">
<concept id="how_the_resource_manager_works">
	<title>How the Resource Manager Works</title>
	<shortdesc>The <keyword keyref="product_name"/> resource manager prevents the cache from
		consuming too much memory by evicting old data. If the garbage collector is unable to keep
		up, the resource manager refuses additions to the cache until the collector has freed an
		adequate amount of memory. </shortdesc>
	<conbody>
		<section id="section_53E80B61991447A2915E8A754383B32D">
			<p>You can use the resource manager in any <keyword keyref="product_name_long"/> member,
				but you may not want to use it everywhere. For some members it might be better to
				occasionally restart after a hang or OME crash than to evict data and/or refuse
				distributed caching activities. Also, members that do not risk running past their
				memory limits would not benefit from the overhead required to run the resource
				manager. Cache servers are often configured to use the manager because they
				generally host more data and have more data activity than other members, requiring
				greater responsiveness in data cleanup and collection. </p>
			<p>The resource manager has two threshold settings, each expressed as a percentage of
				the total tenured heap. Both are disabled by default. <ol
					id="ol_AC9431F3040B4C5499FE2F1FC2A8EFF5">
					<li id="li_2C361A1F3D9549ABA34B3770894599F0"><b>Eviction Threshold</b>. Above
						this, the manager orders evictions for all regions with eviction-attributes
						set to lru-heap-percentage. This prompts dedicated background evictions,
						independent of any application threads and it also tells all application
						threads adding data to the regions to evict at least as much data as they
						add. The JVM garbage collector removes the evicted data, reducing heap use.
						The evictions continue until the manager determines that heap use is again
						below the eviction threshold. </li>
					<li id="li_0B2E41AD95624A6CAD079D39C5661B10"><b>Critical Threshold</b>. Above
						this, all activity that might add data to the cache is refused. This
						threshold is set above the eviction threshold and is intended to allow the
						eviction and GC work to catch up. This JVM, all other JVMs in the
						distributed system, and all clients to the system receive LowMemoryException
						for operations that would add to this critical member's heap consumption.
						Activities that fetch or reduce data are allowed. For a list of refused
						operations, see the Javadocs for the <codeph>ResourceManager</codeph> method
							<codeph>setCriticalHeapPercentage</codeph>. </li>
				</ol>
			</p>
			<image href="../../images/DataManagement-9.gif"
				id="image_C3568D47EE1B4F2C9F0742AE9C291BF1" placement="break"/>
			<p>When heap use passes the eviction threshold, in either direction, the manager logs an
				info-level message. When it passes the critical threshold, the manager logs an
				error-level message. </p>
			<note>Avoid passing the critical threshold. It might be better than a hang or an OME,
				but the <keyword keyref="product_name"/> member that becomes a critical member is a
				read-only member that refuses cache updates for all of its regions, including
				incoming distributed updates. </note>
			<p>For more information, see
					<codeph>com.gemstone.gemfire.cache.control.ResourceManager</codeph> in the
				online API documentation. </p>
		</section>
		<section id="section_EA5E52E65923486488A71E3E6F0DE9DA">
			<title>How Background Eviction Is Performed</title>
			<p>When the manager kicks off evictions: <ol id="ol_F6A52A2AD78149BBBBACA22BBCA896A9">
					<li id="li_F33A94506303483F96B255BCA0381FA0">From all regions in the local cache
						that are configured for heap LRU eviction, the background eviction manager
						creates a randomized list containing one entry for each partitioned region
						bucket (primary or secondary) and one entry for each non-partitioned region.
						So each partitioned region bucket is treated the same as a single,
						non-partitioned region. </li>
					<li id="li_EADC8BB193D84D6EAFE38EB128724160">The background eviction manager
						starts four evictor threads for each processor on the local machine. The
						manager passes each thread its share of the bucket/region list. The manager
						divides the bucket/region list as evenly as possible by count, and not by
						memory consumption. </li>
					<li id="li_DA9CC2A2F156460487FC142A8EF24BFF">Each thread iterates round-robin
						over its bucket/region list, evicting one LRU entry per bucket/region until
						the resource manager sends a signal to stop evicting. </li>
				</ol>
			</p>
			<p>See also <xref
					href="../../reference/topics/memory_requirements_for_cache_data.xml#calculating_memory_requirements"
					type="concept" format="dita" scope="local"/>. </p>
		</section>
	</conbody>
</concept>
